{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6b02c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report,\n",
    "    roc_auc_score, roc_curve, accuracy_score\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Visualization style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "# Paths\n",
    "data_dir = Path(\"data/processed\")\n",
    "models_dir = Path(\"models\")\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Machine learning\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import precision_recall_curve, auc as auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report, \n",
    "    roc_auc_score, roc_curve, accuracy_score, mean_squared_error, r2_score\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import xgboost as xgb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049ae339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL Feature Importance & SHAP\n",
    "import shap\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Train a quick Random Forest (for feature importance)\n",
    "rf_model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "#Feature importance (tree-based)\n",
    "importances = rf_model.feature_importances_\n",
    "feature_names = X_train.columns\n",
    "feat_imp = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "feat_imp = feat_imp.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "#Plot top 20 features\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feat_imp.head(20), palette='viridis')\n",
    "plt.title(\"Top 20 Feature Importances (Random Forest)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#SHAP Analysis (explain individual predictions)\n",
    "#Use TreeExplainer for RandomForest or XGBoost\n",
    "explainer = shap.TreeExplainer(rf_model)\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "\n",
    "#Summary plot (global feature importance)\n",
    "shap.summary_plot(shap_values[1], X_train, plot_type=\"bar\", max_display=20)\n",
    "\n",
    "#Beeswarm plot (impact of each feature on model output)\n",
    "shap.summary_plot(shap_values[1], X_train, plot_type=\"dot\")\n",
    "\n",
    "print(\"Cell complete: Feature importance and SHAP analysis generated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5b8881",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Threshold Tuning & Model Calibration\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Use trained Random Forest or XGBoost model\n",
    "model_to_tune = rf_model  # or xgb_model if trained\n",
    "y_probs = model_to_tune.predict_proba(X_test)[:,1]\n",
    "\n",
    "\n",
    "#Compute precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_probs)\n",
    "\n",
    "# Compute F1 for each threshold\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall + 1e-6)\n",
    "\n",
    "# Find threshold with highest F1\n",
    "best_idx = np.argmax(f1_scores)\n",
    "best_threshold = thresholds[best_idx]\n",
    "print(f\"Optimal threshold for max F1: {best_threshold:.3f} (F1={f1_scores[best_idx]:.3f})\")\n",
    "\n",
    "#Plot Precision-Recall vs Threshold\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(thresholds, precision[:-1], label=\"Precision\", color='b')\n",
    "plt.plot(thresholds, recall[:-1], label=\"Recall\", color='r')\n",
    "plt.plot(thresholds, f1_scores[:-1], label=\"F1 Score\", color='g')\n",
    "plt.axvline(x=best_threshold, color='k', linestyle='--', label=f\"Best Threshold={best_threshold:.3f}\")\n",
    "plt.xlabel(\"Probability Threshold\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Precision, Recall & F1 vs Threshold\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "#Apply new threshold to predictions\n",
    "y_pred_calibrated = (y_probs >= best_threshold).astype(int)\n",
    "\n",
    "#Evaluate calibrated predictions\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score\n",
    "\n",
    "print(\"Classification Report (Calibrated Threshold):\")\n",
    "print(classification_report(y_test, y_pred_calibrated, target_names=[\"Dismiss\", \"Report\"]))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_calibrated)\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=[\"Pred Dismiss\", \"Pred Report\"],\n",
    "            yticklabels=[\"True Dismiss\", \"True Report\"])\n",
    "plt.title(\"Confusion Matrix (Calibrated Threshold)\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_calibrated):.3f}\")\n",
    "print(f\"ROC AUC: {roc_auc_score(y_test, y_probs):.3f}\")\n",
    "\n",
    "print(\"Cell complete: Model calibrated with optimal threshold for AML use case.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02edec96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed feature dataset (from feature_engineering.ipynb)\n",
    "X = pd.read_parquet(data_dir / \"X_features.parquet\")\n",
    "y = pd.read_parquet(data_dir / \"y_labels.parquet\")\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")\n",
    "\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459b29ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained models from feature_engineering.ipynb\n",
    "rf_model = joblib.load(models_dir / \"random_forest_model.joblib\")\n",
    "xgb_model = joblib.load(models_dir / \"xgboost_model.joblib\")\n",
    "dt_model = joblib.load(models_dir / \"decision_tree_model.joblib\")\n",
    "lr_model = joblib.load(models_dir / \"linear_regression_model.joblib\")\n",
    "\n",
    "models = {\n",
    "    \"Random Forest\": rf_model,\n",
    "    \"XGBoost\": xgb_model,\n",
    "    \"Decision Tree\": dt_model,\n",
    "    \"Linear Regression\": lr_model\n",
    "}\n",
    "\n",
    "print(\"Models loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bba3e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models and store metrics\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X)\n",
    "    if hasattr(model, \"predict_proba\"):  # Tree-based models\n",
    "        y_prob = model.predict_proba(X)[:,1]\n",
    "    else:  # Linear Regression or models without predict_proba\n",
    "        y_prob = y_pred\n",
    "\n",
    "    acc = accuracy_score(y, y_pred)\n",
    "    auc = roc_auc_score(y, y_prob)\n",
    "    \n",
    "    print(f\"\\n{name} Performance:\")\n",
    "    print(classification_report(y, y_pred, target_names=[\"Dismiss\", \"Report\"]))\n",
    "    print(f\"Accuracy: {acc:.3f}\")\n",
    "    print(f\"ROC AUC: {auc:.3f}\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    plt.figure(figsize=(5,4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "                xticklabels=[\"Pred Dismiss\", \"Pred Report\"],\n",
    "                yticklabels=[\"True Dismiss\", \"True Report\"])\n",
    "    plt.title(f\"{name} Confusion Matrix\")\n",
    "    plt.show()\n",
    "    \n",
    "    # ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(y, y_prob)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.plot(fpr, tpr, label=f\"AUC = {auc:.2f}\")\n",
    "    plt.plot([0,1], [0,1], 'k--')\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"{name} ROC Curve\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": acc,\n",
    "        \"ROC_AUC\": auc\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cba425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize all model metrics\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values(by=\"ROC_AUC\", ascending=False).reset_index(drop=True)\n",
    "print(\"Model Performance Summary:\")\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e444bf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for Random Forest and XGBoost\n",
    "for name, model in [(\"Random Forest\", rf_model), (\"XGBoost\", xgb_model)]:\n",
    "    if hasattr(model, \"feature_importances_\"):\n",
    "        fi = pd.DataFrame({\n",
    "            \"Feature\": X.columns,\n",
    "            \"Importance\": model.feature_importances_\n",
    "        }).sort_values(by=\"Importance\", ascending=False)\n",
    "        \n",
    "        plt.figure(figsize=(8,6))\n",
    "        sns.barplot(x=\"Importance\", y=\"Feature\", data=fi)\n",
    "        plt.title(f\"{name} Feature Importance\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d535bdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "y_prob_rf = rf_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob_rf))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=[\"Pred Dismiss\", \"Pred Report\"],\n",
    "            yticklabels=[\"True Dismiss\", \"True Report\"])\n",
    "plt.title(\"Random Forest Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd826cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- XGBoost Classifier ---\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use scaled data if already prepared (X_train_scaled, X_test_scaled)\n",
    "# Otherwise, fall back to X_train and X_test\n",
    "X_train_final = X_train_scaled if 'X_train_scaled' in locals() else X_train\n",
    "X_test_final = X_test_scaled if 'X_test_scaled' in locals() else X_test\n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=8,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric=\"logloss\"\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train_final, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_xgb = xgb_model.predict(X_test_final)\n",
    "y_prob_xgb = xgb_model.predict_proba(X_test_final)[:, 1]\n",
    "\n",
    "# Evaluation\n",
    "print(\"XGBoost Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_xgb, target_names=[\"Dismiss\", \"Report\"]))\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_xgb):.3f}\")\n",
    "print(f\"ROC AUC: {roc_auc_score(y_test, y_prob_xgb):.3f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_xgb = confusion_matrix(y_test, y_pred_xgb)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm_xgb, annot=True, fmt='d', cmap='Greens', cbar=False,\n",
    "            xticklabels=[\"Pred Dismiss\", \"Pred Report\"],\n",
    "            yticklabels=[\"True Dismiss\", \"True Report\"])\n",
    "plt.title(\"XGBoost Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8fa1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_dt = dt_model.predict(X_test)\n",
    "y_prob_dt = dt_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(\"Decision Tree Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_dt))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_dt))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob_dt))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_dt = confusion_matrix(y_test, y_pred_dt)\n",
    "sns.heatmap(cm_dt, annot=True, fmt='d', cmap='Oranges', cbar=False,\n",
    "            xticklabels=[\"Pred Dismiss\", \"Pred Report\"],\n",
    "            yticklabels=[\"True Dismiss\", \"True Report\"])\n",
    "plt.title(\"Decision Tree Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc52799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression (predict probability of being Report)\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_lr = lr_model.predict(X_test_scaled)\n",
    "# Convert probabilities to labels\n",
    "y_pred_lr_label = (y_pred_lr > 0.5).astype(int)\n",
    "\n",
    "print(\"Linear Regression Evaluation:\")\n",
    "print(classification_report(y_test, y_pred_lr_label))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_lr_label))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_pred_lr))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_lr = confusion_matrix(y_test, y_pred_lr_label)\n",
    "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Purples', cbar=False,\n",
    "            xticklabels=[\"Pred Dismiss\", \"Pred Report\"],\n",
    "            yticklabels=[\"True Dismiss\", \"True Report\"])\n",
    "plt.title(\"Linear Regression Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "universal_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
