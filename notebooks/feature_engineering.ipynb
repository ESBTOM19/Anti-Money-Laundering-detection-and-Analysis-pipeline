{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e029125d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report, \n",
    "    roc_auc_score, roc_curve, accuracy_score, mean_squared_error, r2_score\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import xgboost as xgb\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0cf958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processed parquet samples\n",
    "alerts = pd.read_parquet(\"data/processed/synthetic_alerts_sample.parquet\")\n",
    "transactions = pd.read_parquet(\"data/processed/synthetic_transactions_sample.parquet\")\n",
    "\n",
    "print(f\"Alerts dataset shape: {alerts.shape}\")\n",
    "print(f\"Transactions dataset shape: {transactions.shape}\")\n",
    "\n",
    "# Merge transactions and alerts by AlertID\n",
    "merged = transactions.merge(alerts, on=\"AlertID\", how=\"inner\")\n",
    "print(f\"Merged dataset shape: {merged.shape}\")\n",
    "\n",
    "# Convert text labels to binary format\n",
    "merged['Label'] = merged['Outcome'].map({'Report': 1, 'Dismiss': 0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c13023c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of feature engineering\n",
    "# Add new features based on existing columns\n",
    "merged['Size_log'] = np.log1p(merged['Size'])      # Log-transform transaction size\n",
    "merged['Size_sqrt'] = np.sqrt(merged['Size'])     # Square root transform\n",
    "\n",
    "# More features could be added here, e.g., time-based or categorical encoding\n",
    "# For demonstration, we'll use only these numeric features\n",
    "features = ['Size', 'Size_log', 'Size_sqrt']\n",
    "X = merged[features]\n",
    "y = merged['Label']\n",
    "\n",
    "print(\"Feature sample:\")\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2aaba15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale features for models that require it (optional for tree-based models)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]}, Testing samples: {X_test.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dab0447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "y_prob_rf = rf_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob_rf))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=[\"Pred Dismiss\", \"Pred Report\"],\n",
    "            yticklabels=[\"True Dismiss\", \"True Report\"])\n",
    "plt.title(\"Random Forest Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c31dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "y_prob_xgb = xgb_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(\"XGBoost Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob_xgb))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_xgb = confusion_matrix(y_test, y_pred_xgb)\n",
    "sns.heatmap(cm_xgb, annot=True, fmt='d', cmap='Greens', cbar=False,\n",
    "            xticklabels=[\"Pred Dismiss\", \"Pred Report\"],\n",
    "            yticklabels=[\"True Dismiss\", \"True Report\"])\n",
    "plt.title(\"XGBoost Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7116d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_dt = dt_model.predict(X_test)\n",
    "y_prob_dt = dt_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(\"Decision Tree Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_dt))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_dt))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob_dt))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_dt = confusion_matrix(y_test, y_pred_dt)\n",
    "sns.heatmap(cm_dt, annot=True, fmt='d', cmap='Oranges', cbar=False,\n",
    "            xticklabels=[\"Pred Dismiss\", \"Pred Report\"],\n",
    "            yticklabels=[\"True Dismiss\", \"True Report\"])\n",
    "plt.title(\"Decision Tree Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2735ad25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression (predict probability of being Report)\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_lr = lr_model.predict(X_test_scaled)\n",
    "# Convert probabilities to labels\n",
    "y_pred_lr_label = (y_pred_lr > 0.5).astype(int)\n",
    "\n",
    "print(\"Linear Regression Evaluation:\")\n",
    "print(classification_report(y_test, y_pred_lr_label))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_lr_label))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_pred_lr))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_lr = confusion_matrix(y_test, y_pred_lr_label)\n",
    "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Purples', cbar=False,\n",
    "            xticklabels=[\"Pred Dismiss\", \"Pred Report\"],\n",
    "            yticklabels=[\"True Dismiss\", \"True Report\"])\n",
    "plt.title(\"Linear Regression Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093804ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,6))\n",
    "\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_prob_rf)\n",
    "fpr_xgb, tpr_xgb, _ = roc_curve(y_test, y_prob_xgb)\n",
    "fpr_dt, tpr_dt, _ = roc_curve(y_test, y_prob_dt)\n",
    "fpr_lr, tpr_lr, _ = roc_curve(y_test, y_pred_lr)\n",
    "\n",
    "plt.plot(fpr_rf, tpr_rf, label=f\"Random Forest (AUC={roc_auc_score(y_test, y_prob_rf):.2f})\")\n",
    "plt.plot(fpr_xgb, tpr_xgb, label=f\"XGBoost (AUC={roc_auc_score(y_test, y_prob_xgb):.2f})\")\n",
    "plt.plot(fpr_dt, tpr_dt, label=f\"Decision Tree (AUC={roc_auc_score(y_test, y_prob_dt):.2f})\")\n",
    "plt.plot(fpr_lr, tpr_lr, label=f\"Linear Regression (AUC={roc_auc_score(y_test, y_pred_lr):.2f})\")\n",
    "\n",
    "plt.plot([0,1], [0,1], 'k--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve Comparison\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "universal_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
