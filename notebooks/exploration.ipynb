{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f987a8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads necessary libraries for data analysis and visualization\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report, \n",
    "    roc_auc_score, roc_curve, accuracy_score\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.style.use('seaborn-v0_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f415e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data (small parquet samples to avoid lag)\n",
    "alerts = pd.read_parquet(\"data/processed/synthetic_alerts_sample.parquet\")\n",
    "transactions = pd.read_parquet(\"data/processed/synthetic_transactions_sample.parquet\")\n",
    "\n",
    "print(f\"Data loaded successfully: {alerts.shape[0]} alerts, {transactions.shape[0]} transactions.\\n\")\n",
    "print(\"Alerts dataset shape:\", alerts.shape)\n",
    "print(\"Transactions dataset shape:\", transactions.shape)\n",
    "\n",
    "alerts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6edc4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge transactions and alerts by AlertID\n",
    "#this is an inner join to ensure only transactions with corresponding alerts are included\n",
    "merged = transactions.merge(alerts, on=\"AlertID\", how=\"inner\")\n",
    "\n",
    "print(\"Merged dataset shape:\", merged.shape)\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09beea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text labels to binary format\n",
    "# 'Report' = 1 (suspicious), 'Dismiss' = 0 (normal)\n",
    "merged['Label'] = merged['Outcome'].map({'Report': 1, 'Dismiss': 0})\n",
    "\n",
    "# Basic cleanup and feature selection\n",
    "X = merged[['Size']]\n",
    "y = merged['Label']\n",
    "\n",
    "print(X.head())\n",
    "print(y.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a09441a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have a saved model:\n",
    "# from joblib import load\n",
    "# model = load('results/models/aml_model.joblib')\n",
    "# y_pred = model.predict(X)\n",
    "\n",
    "# TEMP: Simulate predictions using random probability\n",
    "np.random.seed(42)\n",
    "merged['Predicted_Prob'] = np.random.rand(len(merged))\n",
    "merged['Predicted_Label'] = (merged['Predicted_Prob'] > 0.5).astype(int)\n",
    "\n",
    "y_pred = merged['Predicted_Label']\n",
    "y_prob = merged['Predicted_Prob']\n",
    "#this simulates model predictions using random probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0c50d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this evaluates model performance using various metrics and numerically shows how good the model is at detecting suspicious activities\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y, y_pred, target_names=[\"Dismiss\", \"Report\"]))\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y, y_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b4424d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#builds a confusion matrix visualizing the performance of the classification model using a heatmap \n",
    "cm = confusion_matrix(y, y_pred)\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=[\"Pred Dismiss\", \"Pred Report\"],\n",
    "            yticklabels=[\"True Dismiss\", \"True Report\"])\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "#helps identify where the model is making correct and incorrect predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41645383",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this builds and visualizes a ROC curve to evaluate model performance showing the trade-off between true positive rate and false positive rate\n",
    "#shows AUC score indicating model's ability to distinguish between classes. A higher AUC indicates better performance\n",
    "fpr, tpr, _ = roc_curve(y, y_prob)\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {roc_auc_score(y, y_prob):.2f}\")\n",
    "plt.plot([0,1], [0,1], 'k--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bea2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this summarizes key metrics and provides insights for understanding of model performance\n",
    "print(f\"Evaluation complete.\\n\")\n",
    "print(f\"Accuracy: {accuracy_score(y, y_pred):.3f}\")\n",
    "print(f\"AUC: {roc_auc_score(y, y_prob):.3f}\")\n",
    "print(\"\\nInsights:\")\n",
    "print(\"- 'Report' represents suspicious alerts flagged by the system.\")\n",
    "print(\"- Focus on reducing False Negatives (missed suspicious activities).\")\n",
    "print(\"- If model predicts too many 'Reports', precision drops â€” adjust thresholds.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "universal_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
